{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import neat as neat\n",
    "from pureples.shared.visualize import draw_net\n",
    "from pureples.shared.substrate import Substrate\n",
    "from pureples.es_hyperneat.es_hyperneat import ESNetwork\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test effects of leakage, there are two sets of data. One scaled on the entire dataset, and one scaled on the training set only.\n",
    "\n",
    "# Load datasets scaled on entire dataset\n",
    "X_train_full = pd.read_pickle('../data/train_test_sets/X_train_full.pkl')\n",
    "X_test_full = pd.read_pickle('../data/train_test_sets/X_test_full.pkl')\n",
    "y_train_full = pd.read_pickle('../data/train_test_sets/y_train_full.pkl')\n",
    "y_test_full = pd.read_pickle('../data/train_test_sets/y_test_full.pkl')\n",
    "\n",
    "# Load datasets scaled on training set only\n",
    "X_train_scale = pd.read_pickle('../data/train_test_sets/X_train_scale.pkl')\n",
    "X_test_scale = pd.read_pickle('../data/train_test_sets/X_test_scale.pkl')\n",
    "y_train_scale = pd.read_pickle('../data/train_test_sets/y_train_scale.pkl')\n",
    "y_test_scale = pd.read_pickle('../data/train_test_sets/y_test_scale.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results\n",
    "full_results = {}\n",
    "scale_results = {}\n",
    "tied_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models to test will be:\n",
    "- Decision Tree Classifier\n",
    "- Decision Tree Regressor\n",
    "- Gradient Boosting Regression\n",
    "- K Nearest Neighbors\n",
    "- K Nearest Neighbors Grid Search\n",
    "- Logistic Regression\n",
    "- Neat Neural Network\n",
    "- Random Forest Classifier\n",
    "- Random Forest Grid Search\n",
    "- Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(X_train, X_test, y_train, y_test, depth, criterion, d_type):\n",
    "    dtc = DecisionTreeClassifier(max_depth=depth, criterion=criterion)\n",
    "    dtc.fit(X_train, y_train)\n",
    "    dtc_pred = dtc.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, dtc_pred)\n",
    "    f1 = f1_score(y_test, dtc_pred)\n",
    "    return dtc_pred, accuracy, d_type, f1\n",
    "\n",
    "def decision_tree(X_train, X_test, y_train, y_test, scaled):\n",
    "    depths = [None, 3, 5, 10]\n",
    "    criterions = ['entropy', 'gini']\n",
    "    models = []\n",
    "    f1_scores = []\n",
    "    for depth in depths:\n",
    "        for criterion in criterions:\n",
    "            d_type = 'depth of ' + str(depth) + ' and the ' + criterion + ' criterion'\n",
    "            dtc_model = decision_tree_classifier(X_train, X_test, y_train, y_test, depth=depth, criterion=criterion, d_type=d_type)\n",
    "            models.append(dtc_model)\n",
    "            f1_scores.append(dtc_model[3])\n",
    "    # Find the best F1 score\n",
    "    best_f1 = f1_scores.index(max(f1_scores))\n",
    "    # Find best model\n",
    "    best_model = models[best_f1]\n",
    "    acc = round((best_model[1] * 100), 3)\n",
    "    f1 = round(best_model[3], 3)\n",
    "    print('Decision Tree Classifier ' + scaled + ' with a', best_model[2], 'has an f1-score of', f1, 'and had the best accuracy of ' + str(acc) + '.')\n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, best_model[0]))\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, best_model[0]), '\\n')\n",
    "    return {'Accuracy': acc, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Scaled on Entire Dataset with a depth of 10 and the gini criterion has an f1-score of 0.892 and had the best accuracy of 92.982.\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.92      0.95        79\n",
      "         1.0       0.85      0.94      0.89        35\n",
      "\n",
      "    accuracy                           0.93       114\n",
      "   macro avg       0.91      0.93      0.92       114\n",
      "weighted avg       0.93      0.93      0.93       114\n",
      "\n",
      "Confusion Matrix: \n",
      " [[73  6]\n",
      " [ 2 33]] \n",
      "\n",
      "Decision Tree Classifier Scaled on Training Data with a depth of 5 and the gini criterion has an f1-score of 0.95 and had the best accuracy of 95.614.\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        61\n",
      "           1       1.00      0.91      0.95        53\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.96       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Confusion Matrix: \n",
      " [[61  0]\n",
      " [ 5 48]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "full = decision_tree(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = decision_tree(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Decision Tree Classifier'] = full\n",
    "scale_results['Decision Tree Classifier'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "def dt_regressor(X, y, depth):\n",
    "    dtr = DecisionTreeRegressor(max_depth=depth)\n",
    "    dtr.fit(X, y)\n",
    "    return dtr\n",
    "def decision_tree_regressor(X_train, X_test, y_train, y_test, scaled):\n",
    "    n_regressors = [3, 5, 7]\n",
    "    types = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    for depth in range(1, 10):\n",
    "        for n in n_regressors:\n",
    "            trees = []\n",
    "            type = 'depth of ' + str(depth) + ' and ' + str(n) + ' regressors'\n",
    "            X = X_train\n",
    "            y = y_train\n",
    "            dtr_model = dt_regressor(X, y, depth=depth)\n",
    "            trees.append(dtr_model)\n",
    "            y_pred = dtr_model.predict(X)\n",
    "            for i in range(n-1):\n",
    "                dtr_model = dt_regressor(X, y=y_pred, depth=depth)\n",
    "                trees.append(dtr_model)\n",
    "            dtr_pred = sum([tree.predict(X_test) for tree in trees])\n",
    "            # scale dtr_pred to 0-1 based value is above or below 0.5\n",
    "            dtr_pred = np.where(dtr_pred > 0.5, 1, 0)\n",
    "            accuracy = accuracy_score(y_test, dtr_pred)\n",
    "            types.append(type)\n",
    "            accuracies.append(accuracy)\n",
    "            f1 = f1_score(y_test, dtr_pred)\n",
    "            f1_scores.append(f1)\n",
    "    # Find the best F1 score\n",
    "    best_f1 = f1_scores.index(max(f1_scores))\n",
    "    f1 = round(f1_scores[best_f1], 3)\n",
    "    # Find accuracy\n",
    "    best_accuracy = accuracies[best_f1]\n",
    "    accuracy = round((best_accuracy * 100), 3)\n",
    "    # Find the best type\n",
    "    best_type = types[best_f1]\n",
    "    print(f'Decision Tree Regressor {scaled} with a', best_type, '\\n',\n",
    "          'has an f1-score of', f1, 'and had the best accuracy of ' + str(accuracy) + '.', '\\n')\n",
    "    return {'Accuracy': accuracy, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor Scaled on Entire Dataset with a depth of 5 and 3 regressors \n",
      " has an f1-score of 0.88 and had the best accuracy of 92.105. \n",
      "\n",
      "Decision Tree Regressor Scaled on Training Data with a depth of 3 and 3 regressors \n",
      " has an f1-score of 0.962 and had the best accuracy of 96.491. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "full = decision_tree_regressor(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = decision_tree_regressor(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Decision Tree Regressor'] = full\n",
    "scale_results['Decision Tree Regressor'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "def Gradient_Boosting_Classifier(X_train, X_test, y_train, y_test, scaled):\n",
    "    learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "    models = {}\n",
    "    for depth in range(1, 10):\n",
    "        for learning_rate in learning_rates:\n",
    "            gb = GradientBoostingClassifier(n_estimators=1, learning_rate = learning_rate, max_features=2, max_depth = depth, warm_start=True)\n",
    "            for n_estimators in range(1, 20):\n",
    "                gb.n_estimators = n_estimators\n",
    "                gb.fit(X_train, y_train)\n",
    "                gb_pred = gb.predict(X_test)\n",
    "                accuracy = gb.score(X_test, y_test)\n",
    "                accuracy = round((accuracy * 100), 3)\n",
    "                f1 = f1_score(y_test, gb_pred)\n",
    "                f1 = round(f1, 3)\n",
    "                # Store f1_score, accuracy, depth, learning_rate, n_estimators\n",
    "                models[f1] = [depth, learning_rate, n_estimators, accuracy, gb_pred]\n",
    "\n",
    "    # Find best accuracy\n",
    "    best_f1 = max(models.keys())\n",
    "\n",
    "    print(f'Gradient Boosting {scaled} with a depth of', models[best_f1][0], ', learning rate of', \n",
    "          models[best_f1][1], ', and n_estimators of', models[best_f1][2], '\\n',\n",
    "          'had an f1-score of', best_f1, 'and had the best accuracy of ' + str(models[best_f1][3]) + '.', '\\n')\n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, models[best_f1][4]))\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, models[best_f1][4]), '\\n')\n",
    "    print(models[best_f1][4])\n",
    "    return {'Accuracy': models[best_f1][3], 'F1-Score': best_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Scaled on Entire Dataset with a depth of 9 , learning rate of 0.25 , and n_estimators of 12 \n",
      " had an f1-score of 0.96 and had the best accuracy of 97.368. \n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98        77\n",
      "         1.0       0.95      0.97      0.96        37\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "Confusion Matrix: \n",
      " [[75  2]\n",
      " [ 1 36]] \n",
      "\n",
      "[0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "Gradient Boosting Scaled on Training Data with a depth of 9 , learning rate of 0.05 , and n_estimators of 19 \n",
      " had an f1-score of 1.0 and had the best accuracy of 100.0. \n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        72\n",
      "           1       1.00      1.00      1.00        42\n",
      "\n",
      "    accuracy                           1.00       114\n",
      "   macro avg       1.00      1.00      1.00       114\n",
      "weighted avg       1.00      1.00      1.00       114\n",
      "\n",
      "Confusion Matrix: \n",
      " [[72  0]\n",
      " [ 0 42]] \n",
      "\n",
      "[0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1\n",
      " 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "full = Gradient_Boosting_Classifier(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Gradient_Boosting_Classifier(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Gradient Boosting Classifier'] = full\n",
    "scale_results['Gradient Boosting Classifier'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_N_N(X_train, X_test, y_train, y_test, scaled):\n",
    "\n",
    "    acc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(1,20):\n",
    "\n",
    "        knn = KNeighborsClassifier(i)\n",
    "        knn.fit(X_train,y_train)\n",
    "        acc_scores.append(knn.score(X_test,y_test))\n",
    "        knn_pred = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, knn_pred)\n",
    "        f1_scores.append(f1)\n",
    "    max_f1_score = max(f1_scores)\n",
    "    optimal_k = f1_scores.index(max_f1_score) + 1\n",
    "    f1 = round(max_f1_score, 3)\n",
    "    acc = round((acc_scores[optimal_k] * 100), 3)\n",
    "\n",
    "    print(f'KNN {scaled} with a k of', optimal_k, 'has an f1-score of',\n",
    "           f1, 'and had the best accuracy of ' + str(acc) + '.', '\\n')\n",
    "    return {'Accuracy': acc, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Scaled on Entire Dataset with a k of 4 has an f1-score of 0.941 and had the best accuracy of 92.105. \n",
      "\n",
      "KNN Scaled on Training Data with a k of 13 has an f1-score of 0.962 and had the best accuracy of 95.614. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full = K_N_N(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = K_N_N(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['K Nearest Neighbors'] = full\n",
    "scale_results['K Nearest Neighbors'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_Grid_Search(X_train, X_test, y_train, y_test, scaled):\n",
    "    knn = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors': np.arange(1, 25)}\n",
    "    knn_gscv = GridSearchCV(knn, param_grid, cv=5)\n",
    "    knn_gscv.fit(X_train, y_train)\n",
    "    f1 = f1_score(y_test, knn_gscv.predict(X_test))\n",
    "    f1 = round(f1, 3)\n",
    "    acc = round((knn_gscv.best_score_ * 100), 3)\n",
    "\n",
    "    print(f'KNN Grid Search {scaled} with a k of', knn_gscv.best_params_['n_neighbors'], 'has an f1-score of',\n",
    "              f1, 'and had the best accuracy of ' + str(acc) + '.', '\\n')\n",
    "    return {'Accuracy': acc, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Grid Search Scaled on Entire Dataset with a k of 5 has an f1-score of 0.88 and had the best accuracy of 94.945. \n",
      "\n",
      "KNN Grid Search Scaled on Training Data with a k of 12 has an f1-score of 0.952 and had the best accuracy of 94.725. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full = KNN_Grid_Search(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = KNN_Grid_Search(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['K Nearest Neighbors Grid Search'] = full\n",
    "scale_results['K Nearest Neighbors Grid Search'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, X_test, y_train, y_test, scaled):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, lr_pred)\n",
    "    accuracy = round((accuracy * 100), 3)\n",
    "    f1 = f1_score(y_test, lr_pred)\n",
    "    f1 = round(f1, 3)\n",
    "    print(f'Logistic Regression {scaled} has an f1-score of', f1,\n",
    "           'and had the best accuracy of ' + str(accuracy) + '.', '\\n')\n",
    "    return {'Accuracy': accuracy, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scaled on Entire Dataset has an f1-score of 0.899 and had the best accuracy of 93.86. \n",
      "\n",
      "Logistic Regression Scaled on Training Data has an f1-score of 0.962 and had the best accuracy of 96.491. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full = logistic_regression(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = logistic_regression(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Logistic Regression'] = full\n",
    "scale_results['Logistic Regression'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neat Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_neat = X_train_scale.to_numpy()\n",
    "y_train_neat = y_train_scale.to_numpy()\n",
    "X_test_neat = X_test_scale.to_numpy()\n",
    "y_test_neat = y_test_scale.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def elu(z):\n",
    "    return z if z > 0.0 else math.exp(z) - 1\n",
    "\n",
    "def selu(z):\n",
    "    lam = 1.0507009873554804934193349852946\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    return lam * z if z > 0.0 else lam * alpha * (math.exp(z) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, genome, config):\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        self.fitness = None\n",
    "        self.net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "    def activate(self, X):\n",
    "        return self.net.activate(X)\n",
    "    def predict(self, X):\n",
    "        return np.array([self.activate(x) for x in X])\n",
    "\n",
    "def eval_genomes(genomes, config):\n",
    "    #global stag_count\n",
    "    #stag_count += 1\n",
    "    #if stag_count == 30:\n",
    "    #    config.pop_size = 2000\n",
    "    networks = []\n",
    "    for genome_id, genome in genomes:\n",
    "        networks.append(Network(genome, config))\n",
    "    for network in networks:\n",
    "        network.fitness = 0\n",
    "    for network in networks:\n",
    "        predictions = [np.argmax(network.activate(xi)) for xi in X_train_neat]\n",
    "        network.fitness = (f1_score(y_train_neat, predictions)*100)\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness = next(network.fitness for network in networks if network.genome == genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neat(X_test_neat, y_test_neat, scaled):\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction, neat.DefaultSpeciesSet, neat.DefaultStagnation, '../neat_config.txt')\n",
    "    config.genome_config.add_activation('elu', elu)\n",
    "    config.genome_config.add_activation('selu', selu)\n",
    "    p = neat.Population(config)\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    winner = p.run(eval_genomes, 500)\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    predictions = [np.argmax(winner_net.activate(xi)) for xi in X_test_neat]\n",
    "    accuracy = accuracy_score(y_test_neat, predictions)\n",
    "    accuracy = round((accuracy * 100), 3)\n",
    "    f1 = f1_score(y_test_neat, predictions)\n",
    "    f1 = round(f1, 3)\n",
    "\n",
    "    print(f'NEAT {scaled} has an f1 score of', f1, 'and has an accuracy of ' + str(accuracy) + '.')\n",
    "    return winner\n",
    "\n",
    "stag_count = 0\n",
    "scale = run_neat(X_test_neat, y_test_neat, scaled='Scaled on Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEAT Scaled on Training Data has an f1 score of 0.935 and has an accuracy of 93.86.\n"
     ]
    }
   ],
   "source": [
    "config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction, neat.DefaultSpeciesSet, neat.DefaultStagnation, '../neat_config.txt')\n",
    "config.genome_config.add_activation('elu', elu)\n",
    "config.genome_config.add_activation('selu', selu)\n",
    "winner_net = neat.nn.FeedForwardNetwork.create(scale, config)\n",
    "predictions = [np.argmax(winner_net.activate(xi)) for xi in X_test_neat]\n",
    "accuracy = round((accuracy_score(y_test_neat, predictions) * 100),3)\n",
    "f1 = round(f1_score(y_test_neat, predictions), 3)\n",
    "print(f'NEAT Scaled on Training Data has an f1 score of', f1, 'and has an accuracy of ' + str(accuracy) + '.')\n",
    "scale_results['NEAT'] = {'Accuracy': accuracy, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Classifier(X_train, X_test, y_train, y_test, scaled):\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, rf_pred)\n",
    "    accuracy = round((accuracy * 100), 3)\n",
    "    f1 = f1_score(y_test, rf_pred)\n",
    "    f1 = round(f1, 3)\n",
    "    print(f'Random Forest Classifier {scaled} has an f1-score of', f1,\n",
    "           'and had the best accuracy of ' + str(accuracy) + '.', '\\n')\n",
    "    return {'Accuracy': accuracy, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Scaled on Entire Dataset has an f1-score of 0.904 and had the best accuracy of 93.86. \n",
      "\n",
      "Random Forest Classifier Scaled on Training Data has an f1-score of 0.961 and had the best accuracy of 96.491. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full = Random_Forest_Classifier(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Random_Forest_Classifier(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Random Forest Classifier'] = full\n",
    "scale_results['Random Forest Classifier'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Grid_Search(X_train, X_test, y_train, y_test, scaled):\n",
    "    rf = RandomForestClassifier()\n",
    "    param_grid = {'n_estimators': np.arange(1, 25)}\n",
    "    rf_gscv = GridSearchCV(rf, param_grid, cv=5)\n",
    "    rf_gscv.fit(X_train, y_train)\n",
    "    f1 = f1_score(y_test, rf_gscv.predict(X_test))\n",
    "    f1 = round(f1, 3)\n",
    "    acc = round((rf_gscv.best_score_ * 100), 3)\n",
    "\n",
    "    print(f'Random Forest Grid Search {scaled} with a n_estimator of', rf_gscv.best_params_['n_estimators'], \n",
    "          '\\nhas an f1-score of', f1, 'and had the best accuracy of ' + str(acc) + '.', '\\n')\n",
    "    return {'Accuracy': acc, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Grid Search Scaled on Entire Dataset with a n_estimator of 11 \n",
      "has an f1-score of 0.901 and had the best accuracy of 94.505. \n",
      "\n",
      "Random Forest Grid Search Scaled on Training Data with a n_estimator of 17 \n",
      "has an f1-score of 0.971 and had the best accuracy of 93.407. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full = Random_Forest_Grid_Search(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Random_Forest_Grid_Search(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Random Forest Grid Search'] = full\n",
    "scale_results['Random Forest Grid Search'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Regressor(X_train, X_test, y_train, y_test, scaled):\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_pred = np.where(rf_pred > 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_test, rf_pred)\n",
    "    accuracy = round((accuracy * 100), 3)\n",
    "    f1 = f1_score(y_test, rf_pred)\n",
    "    f1 = round(f1, 3)\n",
    "    print(f'Random Forest Regressor {scaled} has an f1-score of', f1,\n",
    "           'and had the best accuracy of ' + str(accuracy) + '.', '\\n')\n",
    "    return {'Accuracy': accuracy, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Scaled on Entire Dataset has an f1-score of 0.889 and had the best accuracy of 92.982. \n",
      "\n",
      "Random Forest Regressor Scaled on Training Data has an f1-score of 0.971 and had the best accuracy of 97.368. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full = Random_Forest_Regressor(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Random_Forest_Regressor(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Random Forest Regressor'] = full\n",
    "scale_results['Random Forest Regressor'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train, X_test, y_train, y_test, scaled):\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, svm_pred)\n",
    "    accuracy = round((accuracy * 100), 3)\n",
    "    f1 = f1_score(y_test, svm_pred)\n",
    "    f1 = round(f1, 3)\n",
    "\n",
    "    print(f'Support Vector Machine {scaled} has an f1-score of', f1,\n",
    "              'and had the best accuracy of ' + str(accuracy) + '.', '\\n')\n",
    "    return {'Accuracy': accuracy, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Scaled on Entire Dataset has an f1-score of 0.93 and had the best accuracy of 95.614. \n",
      "\n",
      "Support Vector Machine Scaled on Training Data has an f1-score of 0.923 and had the best accuracy of 92.982. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full = svm(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = svm(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "\n",
    "full_results['Support Vector Machine'] = full\n",
    "scale_results['Support Vector Machine'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Accuracy  F1-Score\n",
      "Model                                              \n",
      "Gradient Boosting Classifier       96.491     0.944\n",
      "Support Vector Machine             95.614     0.930\n",
      "K Nearest Neighbors Grid Search    94.945     0.880\n",
      "Random Forest Grid Search          94.505     0.901\n",
      "Logistic Regression                93.860     0.899\n",
      "Random Forest Classifier           93.860     0.904\n",
      "Decision Tree Classifier           92.982     0.892\n",
      "Random Forest Regressor            92.982     0.889\n",
      "Decision Tree Regressor            92.105     0.880\n",
      "K Nearest Neighbors                92.105     0.941\n"
     ]
    }
   ],
   "source": [
    "sorted_full_results = {k: v for k, v in sorted(full_results.items(), key=lambda item: item[1]['Accuracy'], reverse=True)}\n",
    "full_results = pd.DataFrame.from_dict(sorted_full_results, orient='index')\n",
    "full_results.index.names = ['Model']\n",
    "print(full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Accuracy  F1-Score\n",
      "Model                                              \n",
      "Gradient Boosting Classifier       98.246     0.981\n",
      "Random Forest Regressor            97.368     0.971\n",
      "Decision Tree Regressor            96.491     0.962\n",
      "Logistic Regression                96.491     0.962\n",
      "Random Forest Classifier           96.491     0.961\n",
      "Decision Tree Classifier           95.614     0.950\n",
      "K Nearest Neighbors                95.614     0.962\n",
      "K Nearest Neighbors Grid Search    94.725     0.952\n",
      "NEAT                               93.860     0.935\n",
      "Random Forest Grid Search          93.407     0.971\n",
      "Support Vector Machine             92.982     0.923\n"
     ]
    }
   ],
   "source": [
    "sorted_scale_results = {k: v for k, v in sorted(scale_results.items(), key=lambda item: item[1]['Accuracy'], reverse=True)}\n",
    "scale_results = pd.DataFrame.from_dict(sorted_scale_results, orient='index')\n",
    "scale_results.index.names = ['Model']\n",
    "print(scale_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns I've chosen to use are:\n",
    "- Radius Mean\n",
    "- Texture Mean\n",
    "- Smoothness Mean\n",
    "- Compactness Mean\n",
    "- Concavity Mean\n",
    "- Concave Points Mean\n",
    "- Symmetry Mean\n",
    "- Fractal Dimension Mean\n",
    "\n",
    "Which on average, gives:\n",
    "- Gradient Boosting Classifier with 98.25%\n",
    "- Random Forest Regressor with 97.37%\n",
    "\n",
    "And the following three models tied at 96.49%:\n",
    "- Decision Tree Regressor\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When utilizing similar columns as Kaggle notebook (https://www.kaggle.com/code/priyanka841/breast-cancer-diagnostics-prediction)\n",
    "- Radius Mean\n",
    "- Texture Mean\n",
    "- Smoothness Mean\n",
    "- Compactness Mean\n",
    "- Symmetry Mean\n",
    "- Fractal Dimension Mean\n",
    "- Radius Standard Error\n",
    "- Texture Standard Error\n",
    "- Smoothness Standard Error\n",
    "- Compactness Standard Error\n",
    "- Symmetry Standard Error\n",
    "- Fractal Dimension Standard Error\n",
    "\n",
    "Scaled Results:\n",
    "\n",
    "![](../images/secondary_results.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the models trained on data that was scaled on training data, not the full dataset, score better. Leakage does seem to hinder the test accuracy.\n",
    "\n",
    "My criteria for success was to find a model with higher accuracy than the example notebook by 'priyanka841'. Having originally chosen less columns (just the 'Mean' columns, not including the 'Standard Error'), I matched their highest accuracy for SVM results at 96.49% with 3 separate models. Beyond that, Gradient Boosting Classifier reached 98.25% and Random Forest Regressor reached 97.37%.\n",
    "\n",
    "When adjusting the columns in the dataset to match 'priyanka841' (addition of Standard Error, removing concavity and concave points), Gradiant Boosting Classifier only tied their SVM results of 96.49%.\n",
    "\n",
    "Additionally, the NEAT neural network was tested and reached an accuracy of 93.86%. The caveat was that while the other models trained in less than 30 seconds, NEAT was allowed to train for ~16 minutes. While it's theoretically possible that the accuracy could improve, time invested must be considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
