{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import neat as neat\n",
    "from pureples.shared.visualize import draw_net\n",
    "from pureples.shared.substrate import Substrate\n",
    "from pureples.es_hyperneat.es_hyperneat import ESNetwork\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test effects of leakage, there are two sets of data. One scaled on the entire dataset, and one scaled on the training set only.\n",
    "\n",
    "# Load datasets scaled on entire dataset\n",
    "X_train_full = pd.read_pickle('../data/train_test_sets/X_train_full.pkl')\n",
    "X_test_full = pd.read_pickle('../data/train_test_sets/X_test_full.pkl')\n",
    "y_train_full = pd.read_pickle('../data/train_test_sets/y_train_full.pkl')\n",
    "y_test_full = pd.read_pickle('../data/train_test_sets/y_test_full.pkl')\n",
    "\n",
    "# Load datasets scaled on training set only\n",
    "X_train_scale = pd.read_pickle('../data/train_test_sets/X_train_scale.pkl')\n",
    "X_test_scale = pd.read_pickle('../data/train_test_sets/X_test_scale.pkl')\n",
    "y_train_scale = pd.read_pickle('../data/train_test_sets/y_train_scale.pkl')\n",
    "y_test_scale = pd.read_pickle('../data/train_test_sets/y_test_scale.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results\n",
    "full_results = {}\n",
    "scale_results = {}\n",
    "tied_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models to test will be:\n",
    "- Decision Tree Classifier\n",
    "- Decision Tree Regressor\n",
    "- Gradient Boosting Regression\n",
    "- K Nearest Neighbors\n",
    "- K Nearest Neighbors Grid Search\n",
    "- Logistic Regression\n",
    "- Neat Neural Network\n",
    "- Random Forest Classifier\n",
    "- Random Forest Grid Search\n",
    "- Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(X_train, X_test, y_train, y_test, depth, criterion, type):\n",
    "    dtc = DecisionTreeClassifier(max_depth=depth, criterion=criterion)\n",
    "    dtc.fit(X_train, y_train)\n",
    "    dtc_pred = dtc.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, dtc_pred)\n",
    "    return dtc_pred, accuracy, type\n",
    "\n",
    "def decision_tree(X_train, X_test, y_train, y_test, scaled):\n",
    "    depths = [None, 3, 5, 10]\n",
    "    criterions = ['entropy', 'gini']\n",
    "    models = []\n",
    "    accuracies = []\n",
    "    for depth in depths:\n",
    "        for criterion in criterions:\n",
    "            type = 'depth of ' + str(depth) + ' and the ' + criterion + ' criterion'\n",
    "            dtc_model = decision_tree_classifier(X_train, X_test, y_train, y_test, depth=depth, criterion=criterion, type=type)\n",
    "            models.append(dtc_model)\n",
    "            accuracies.append(dtc_model[1])\n",
    "    # Find best accuracy\n",
    "    best_accuracy = accuracies.index(max(accuracies))\n",
    "    # Find best model\n",
    "    best_model = models[best_accuracy]\n",
    "    print('Decision Tree Classifier ' + scaled + ' with a', best_model[2], 'has the best accuracy of ' + str(best_model[1]) + '.')\n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, best_model[0]))\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, best_model[0]))\n",
    "    print(\"Accuracy:\", best_model[1])\n",
    "    return round((best_model[1] * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Scaled on Entire Dataset with a depth of 3 and the gini criterion has the best accuracy of 0.9473684210526315.\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96        71\n",
      "         1.0       0.95      0.91      0.93        43\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "Confusion Matrix: \n",
      " [[69  2]\n",
      " [ 4 39]]\n",
      "Accuracy: 0.9473684210526315\n",
      "Decision Tree Classifier Scaled on Training Data with a depth of None and the gini criterion has the best accuracy of 0.9385964912280702.\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95        77\n",
      "           1       0.92      0.89      0.90        37\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.93      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n",
      "Confusion Matrix: \n",
      " [[74  3]\n",
      " [ 4 33]]\n",
      "Accuracy: 0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "full = decision_tree(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = decision_tree(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['Decision Tree Classifier'] = full\n",
    "scale_results['Decision Tree Classifier'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "def dt_regressor(X, y, depth):\n",
    "    dtr = DecisionTreeRegressor(max_depth=depth)\n",
    "    dtr.fit(X, y)\n",
    "    return dtr\n",
    "def decision_tree_regressor(X_train, X_test, y_train, y_test, scaled):\n",
    "    n_regressors = [3, 5, 7]\n",
    "    types = []\n",
    "    accuracies = []\n",
    "    for depth in range(1, 10):\n",
    "        for n in n_regressors:\n",
    "            trees = []\n",
    "            type = 'depth of ' + str(depth) + ' and ' + str(n) + ' regressors'\n",
    "            X = X_train\n",
    "            y = y_train\n",
    "            dtr_model = dt_regressor(X, y, depth=depth)\n",
    "            trees.append(dtr_model)\n",
    "            y_pred = dtr_model.predict(X)\n",
    "            for i in range(n-1):\n",
    "                dtr_model = dt_regressor(X, y=y_pred, depth=depth)\n",
    "                trees.append(dtr_model)\n",
    "            dtr_pred = sum([tree.predict(X_test) for tree in trees])\n",
    "            # scale dtr_pred to 0-1 based value is above or below 0.5\n",
    "            dtr_pred = np.where(dtr_pred > 0.5, 1, 0)\n",
    "            accuracy = accuracy_score(y_test, dtr_pred)\n",
    "            types.append(type)\n",
    "            accuracies.append(accuracy)\n",
    "    # Find best accuracy\n",
    "    best_accuracy = max(accuracies)\n",
    "    accuracy_index = accuracies.index(best_accuracy)\n",
    "    # Find the best type\n",
    "    best_type = types[accuracy_index]\n",
    "    print(f'Decision Tree Regressor {scaled} with a', best_type, 'has an accuracy of ' + str(accuracy) + '.')\n",
    "    \n",
    "    return round((best_accuracy * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor Scaled on Entire Dataset with a depth of 7 and 7 regressors has an accuracy of 0.9385964912280702.\n",
      "Decision Tree Regressor Scaled on Training Data with a depth of 5 and 5 regressors has an accuracy of 0.9210526315789473.\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "full = decision_tree_regressor(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = decision_tree_regressor(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['Decision Tree Regressor'] = full\n",
    "    scale_results['Decision Tree Regressor'] = scale\n",
    "elif scale > full:\n",
    "    scale_results['Decision Tree Regressor'] = scale\n",
    "else:\n",
    "    tied_results['Decision Tree Regressor'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "def Gradient_Boosting_Classifier(X_train, X_test, y_train, y_test, scaled):\n",
    "    learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "    models = {}\n",
    "    for depth in range(1, 10):\n",
    "        for learning_rate in learning_rates:\n",
    "            gb = GradientBoostingClassifier(n_estimators=1, learning_rate = learning_rate, max_features=2, max_depth = depth, warm_start=True)\n",
    "            for n_estimators in range(1, 20):\n",
    "                gb.n_estimators = n_estimators\n",
    "                gb.fit(X_train, y_train)\n",
    "                accuracy = gb.score(X_test, y_test)\n",
    "                # Store accuracy, depth, learning rate, and n_estimators\n",
    "                models[accuracy] = [depth, learning_rate, n_estimators]\n",
    "    # Find best accuracy\n",
    "    best_accuracy = max(models.keys())\n",
    "\n",
    "    print(f'Gradient Boosting {scaled} with a depth of', models[best_accuracy][0], ', learning rate of', models[best_accuracy][1], ', and n_estimators of', models[best_accuracy][2], 'has an accuracy of ' + str(accuracy) + '.')\n",
    "\n",
    "    return round((best_accuracy * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Scaled on Entire Dataset with a depth of 7 , learning rate of 0.5 , and n_estimators of 8 has an accuracy of 0.9298245614035088.\n",
      "Gradient Boosting Scaled on Training Data with a depth of 9 , learning rate of 0.25 , and n_estimators of 17 has an accuracy of 0.9385964912280702.\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "full = Gradient_Boosting_Classifier(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Gradient_Boosting_Classifier(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['Gradient Boosting Classifier'] = full\n",
    "    scale_results['Gradient Boosting Classifier'] = scale\n",
    "elif scale > full:\n",
    "    scale_results['Gradient Boosting Classifier'] = scale\n",
    "else:\n",
    "    tied_results['Gradient Boosting Classifier'] = scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting with the best results has inconsistent parameters. During initial testing, results were:\n",
    "- Training Data with a depth of 7 , learning rate of 0.1 , and n_estimators of 10 has an accuracy of 0.9473684210526315.\n",
    "- Training Data with a depth of 8 , learning rate of 0.1 , and n_estimators of 5 has an accuracy of 0.9473684210526315.\n",
    "- Training Data with a depth of 9 , learning rate of 1 , and n_estimators of 19 has an accuracy of 0.9649122807017544.\n",
    "- Entire Dataset with a depth of 9 , learning rate of 0.5 , and n_estimators of 19 has an accuracy of 0.9385964912280702.\n",
    "- Entire Dataset with a depth of 2 , learning rate of 0.25 , and n_estimators of 8 has an accuracy of 0.9298245614035088.\n",
    "- Training Data with a depth of 9 , learning rate of 1 , and n_estimators of 19 has an accuracy of 0.9736842105263158."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_N_N(X_train, X_test, y_train, y_test, scaled):\n",
    "\n",
    "    test_scores = []\n",
    "\n",
    "    for i in range(1,20):\n",
    "\n",
    "        knn = KNeighborsClassifier(i)\n",
    "        knn.fit(X_train,y_train)\n",
    "        test_scores.append(knn.score(X_test,y_test))\n",
    "    max_test_score = max(test_scores)\n",
    "    optimal_k = test_scores.index(max_test_score) + 1\n",
    "    print(f'KNN {scaled} with a k of', optimal_k, 'has an accuracy of ' + str(max_test_score) + '.')\n",
    "    return round((max_test_score * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Scaled on Entire Dataset with a k of 3 has an accuracy of 0.9385964912280702.\n",
      "KNN Scaled on Training Data with a k of 5 has an accuracy of 0.9824561403508771.\n"
     ]
    }
   ],
   "source": [
    "full = K_N_N(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = K_N_N(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['K Nearest Neighbors'] = full\n",
    "    scale_results['K Nearest Neighbors'] = scale\n",
    "elif scale > full:\n",
    "    scale_results['K Nearest Neighbors'] = scale\n",
    "else:\n",
    "    tied_results['K Nearest Neighbors'] = full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors Grid Search\n",
    "We will not track the given accuracy, but useful to compare best estimated number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_Grid_Search(X_train, X_test, y_train, y_test, scaled):\n",
    "    knn = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors': np.arange(1, 25)}\n",
    "    knn_gscv = GridSearchCV(knn, param_grid, cv=5)\n",
    "    knn_gscv.fit(X_train, y_train)\n",
    "    print(f'KNN Grid Search {scaled} with a k of', knn_gscv.best_params_['n_neighbors'], 'has an accuracy of ' + str(knn_gscv.best_score_) + '.')\n",
    "    return round((knn_gscv.best_score_ * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Grid Search Scaled on Entire Dataset with a k of 18 has an accuracy of 0.956043956043956.\n",
      "KNN Grid Search Scaled on Training Data with a k of 21 has an accuracy of 0.945054945054945.\n"
     ]
    }
   ],
   "source": [
    "full = KNN_Grid_Search(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = KNN_Grid_Search(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, X_test, y_train, y_test, scaled):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, lr_pred)\n",
    "    print(f'Logistic Regression {scaled} has an accuracy of ' + str(accuracy) + '.')\n",
    "    return round((accuracy * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scaled on Entire Dataset has an accuracy of 0.9122807017543859.\n",
      "Logistic Regression Scaled on Training Data has an accuracy of 0.9473684210526315.\n"
     ]
    }
   ],
   "source": [
    "full = logistic_regression(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = logistic_regression(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['Logistic Regression'] = full\n",
    "    scale_results['Logistic Regression'] = scale\n",
    "elif scale > full:\n",
    "    scale_results['Logistic Regression'] = scale\n",
    "else:\n",
    "    tied_results['Logistic Regression'] = full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neat Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_neat = X_train_scale.to_numpy()\n",
    "y_train_neat = y_train_scale.to_numpy()\n",
    "X_test_neat = X_test_scale.to_numpy()\n",
    "y_test_neat = y_test_scale.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def elu(z):\n",
    "    return z if z > 0.0 else math.exp(z) - 1\n",
    "\n",
    "def selu(z):\n",
    "    lam = 1.0507009873554804934193349852946\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    return lam * z if z > 0.0 else lam * alpha * (math.exp(z) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, genome, config):\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        self.fitness = None\n",
    "        self.net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "    def activate(self, X):\n",
    "        return self.net.activate(X)\n",
    "    def predict(self, X):\n",
    "        return np.array([self.activate(x) for x in X])\n",
    "\n",
    "stag_count = 0\n",
    "\n",
    "def eval_genomes(genomes, config):\n",
    "    global stag_count\n",
    "    stag_count += 1\n",
    "    if stag_count == 10:\n",
    "        config.stagnation_config.max_stagnation = 15\n",
    "    networks = []\n",
    "    for genome_id, genome in genomes:\n",
    "        networks.append(Network(genome, config))\n",
    "    for network in networks:\n",
    "        network.fitness = 0\n",
    "    for network in networks:\n",
    "        predictions = [np.argmax(network.activate(xi)) for xi in X_train_neat]\n",
    "        network.fitness = accuracy_score(y_train_neat, predictions) * 100\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness = next(network.fitness for network in networks if network.genome == genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neat(X_test_neat, y_test_neat, scaled):\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction, neat.DefaultSpeciesSet, neat.DefaultStagnation, '../neat_config.txt')\n",
    "    config.genome_config.add_activation('elu', elu)\n",
    "    config.genome_config.add_activation('selu', selu)\n",
    "    p = neat.Population(config)\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    winner = p.run(eval_genomes, 500)\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    predictions = [np.argmax(winner_net.activate(xi)) for xi in X_test_neat]\n",
    "    accuracy = accuracy_score(y_test_neat, predictions) * 100\n",
    "    print(f'NEAT {scaled} has an accuracy of ' + str(accuracy) + '.')\n",
    "    return winner\n",
    "\n",
    "\n",
    "scale = run_neat(X_test_neat, y_test_neat, scaled='Scaled on Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction, neat.DefaultSpeciesSet, neat.DefaultStagnation, '../neat_config.txt')\n",
    "config.genome_config.add_activation('elu', elu)\n",
    "config.genome_config.add_activation('selu', selu)\n",
    "winner_net = neat.nn.FeedForwardNetwork.create(scale, config)\n",
    "predictions = [np.argmax(winner_net.activate(xi)) for xi in X_test_neat]\n",
    "accuracy = round((accuracy_score(y_test_neat, predictions) * 100),2)\n",
    "scale_results['NEAT'] = accuracy\n",
    "print(f'NEAT has an accuracy of ' + str(accuracy_score) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Classifier(X_train, X_test, y_train, y_test, scaled):\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, rf_pred)\n",
    "    print(f'Random Forest Classifier {scaled} has an accuracy of ' + str(accuracy) + '.')\n",
    "    return round((accuracy * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Scaled on Entire Dataset has an accuracy of 0.9210526315789473.\n",
      "Random Forest Classifier Scaled on Training Data has an accuracy of 0.9473684210526315.\n"
     ]
    }
   ],
   "source": [
    "full = Random_Forest_Classifier(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Random_Forest_Classifier(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['Random Forest Classifier'] = full\n",
    "    scale_results['Random Forest Classifier'] = scale\n",
    "elif scale > full:\n",
    "    scale_results['Random Forest Classifier'] = scale\n",
    "else:\n",
    "    tied_results['Random Forest Classifier'] = full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Grid Search\n",
    "Similar to KNN Grid Search, we will not track the given accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Grid_Search(X_train, X_test, y_train, y_test, scaled):\n",
    "    rf = RandomForestClassifier()\n",
    "    param_grid = {'n_estimators': np.arange(1, 25)}\n",
    "    rf_gscv = GridSearchCV(rf, param_grid, cv=5)\n",
    "    rf_gscv.fit(X_train, y_train)\n",
    "    print(f'Random Forest Grid Search {scaled} with a n_estimator of', rf_gscv.best_params_['n_estimators'], 'has an accuracy of ' + str(rf_gscv.best_score_) + '.')\n",
    "    return round((rf_gscv.best_score_ * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Grid Search Scaled on Entire Dataset with a n_estimator of 22 has an accuracy of 0.945054945054945.\n",
      "Random Forest Grid Search Scaled on Training Data with a n_estimator of 12 has an accuracy of 0.9494505494505496.\n"
     ]
    }
   ],
   "source": [
    "full = Random_Forest_Grid_Search(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Random_Forest_Grid_Search(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Regressor(X_train, X_test, y_train, y_test, scaled):\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_pred = np.where(rf_pred > 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_test, rf_pred)\n",
    "    print(f'Random Forest Regressor {scaled} has an accuracy of ' + str(accuracy) + '.')\n",
    "    return round((accuracy * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Scaled on Entire Dataset has an accuracy of 0.9298245614035088.\n",
      "Random Forest Regressor Scaled on Training Data has an accuracy of 0.9385964912280702.\n"
     ]
    }
   ],
   "source": [
    "full = Random_Forest_Regressor(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = Random_Forest_Regressor(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['Random Forest Regressor'] = full\n",
    "    scale_results['Random Forest Regressor'] = scale\n",
    "elif scale > full:\n",
    "    scale_results['Random Forest Regressor'] = scale\n",
    "else:\n",
    "    tied_results['Random Forest Regressor'] = full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train, X_test, y_train, y_test, scaled):\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, svm_pred)\n",
    "    print(f'Support Vector Machine {scaled} has an accuracy of ' + str(accuracy) + '.')\n",
    "    return round((accuracy * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Scaled on Entire Dataset has an accuracy of 0.9210526315789473.\n",
      "Support Vector Machine Scaled on Training Data has an accuracy of 0.9649122807017544.\n"
     ]
    }
   ],
   "source": [
    "full = svm(X_train_full, X_test_full, y_train_full, y_test_full, scaled='Scaled on Entire Dataset')\n",
    "scale = svm(X_train_scale, X_test_scale, y_train_scale, y_test_scale, scaled='Scaled on Training Data')\n",
    "if full > scale:\n",
    "    full_results['Support Vector Machine'] = full\n",
    "    scale_results['Support Vector Machine'] = scale\n",
    "elif scale > full:\n",
    "    scale_results['Support Vector Machine'] = scale\n",
    "else:\n",
    "    tied_results['Support Vector Machine'] = full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Results:\n",
      "Decision Tree Classifier has an accuracy of 94.74\n",
      "Decision Tree Regressor has an accuracy of 94.74\n",
      "\n",
      "Scaled Results:\n",
      "Decision Tree Classifier has an accuracy of 93.86\n",
      "Decision Tree Regressor has an accuracy of 92.11\n",
      "Gradient Boosting Classifier has an accuracy of 97.37\n",
      "K Nearest Neighbors has an accuracy of 98.25\n",
      "Logistic Regression has an accuracy of 94.74\n",
      "NEAT has an accuracy of 96.49\n",
      "Random Forest Classifier has an accuracy of 94.74\n",
      "Random Forest Regressor has an accuracy of 93.86\n",
      "Support Vector Machine has an accuracy of 96.49\n",
      "\n",
      "Tied Results:\n"
     ]
    }
   ],
   "source": [
    "# Print the results of full_results\n",
    "print('Full Results:')\n",
    "for key, value in full_results.items():\n",
    "    print(key, 'has an accuracy of', value)\n",
    "\n",
    "# Print the results of scale_results\n",
    "print('\\nScaled Results:')\n",
    "for key, value in scale_results.items():\n",
    "    print(key, 'has an accuracy of', value)\n",
    "\n",
    "# Print the results of tied_results\n",
    "print('\\nTied Results:')\n",
    "for key, value in tied_results.items():\n",
    "    print(key, 'has an accuracy of', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns I've chosen to use are:\n",
    "- Radius Mean\n",
    "- Texture Mean\n",
    "- Smoothness Mean\n",
    "- Compactness Mean\n",
    "- Concavity Mean\n",
    "- Concave Points Mean\n",
    "- Symmetry Mean\n",
    "- Fractal Dimension Mean\n",
    "\n",
    "Which on average, gives:\n",
    "- K Nearest Neighbors with 98.25%\n",
    "- Gradient Boosting Classifier with 97.37%\n",
    "- Support Vector Machine with 96.49%\n",
    "- NEAT Neural Network with 96.49%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When utilizing similar columns as Kaggle notebook (https://www.kaggle.com/code/priyanka841/breast-cancer-diagnostics-prediction)\n",
    "- Radius Mean\n",
    "- Texture Mean\n",
    "- Smoothness Mean\n",
    "- Compactness Mean\n",
    "- Symmetry Mean\n",
    "- Fractal Dimension Mean\n",
    "- Radius Standard Error\n",
    "- Texture Standard Error\n",
    "- Smoothness Standard Error\n",
    "- Compactness Standard Error\n",
    "- Symmetry Standard Error\n",
    "- Fractal Dimension Standard Error\n",
    "\n",
    "Full Results:\n",
    "- (None)\n",
    "\n",
    "Scaled Results:\n",
    "- Decision Tree Classifier has an accuracy of 95.61\n",
    "- Decision Tree Regressor has an accuracy of 96.49\n",
    "- Gradient Boosting Classifier has an accuracy of 99.12\n",
    "- K Nearest Neighbors has an accuracy of 94.73\n",
    "- Logistic Regression has an accuracy of 94.73\n",
    "- Random Forest Classifier has an accuracy of 96.49\n",
    "- Random Forest Regressor has an accuracy of 94.74\n",
    "- Support Vector Machine has an accuracy of 93.86\n",
    "\n",
    "Tied Results:\n",
    "- (None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the models trained on data that was scaled on training data, not the full dataset, score better. Leakage does seem to hinder the test accuracy.\n",
    "\n",
    "My criteria for success was to find a model with higher accuracy than the example notebook by 'priyanka841'. Having originally chosen less columns (just the 'Mean' columns, not including the 'Standard Error'), I matched the SVM results at 96.49%. Beyond that, K Nearest Columns scored 98.25% and Gradient Boosting Classifier scored 97.37%.\n",
    "\n",
    "When adjusting the columns in the dataset to match 'priyanka841' (addition of Standard Error, removing concavity and concave points), Gradiant Boosting Classifier reached an impressive 99.12%.\n",
    "\n",
    "Additionally, the NEAT neural network was tested and was tied in 3rd place with 96.49%. The caveat, was that while the other models trained in less than 30 seconds, NEAT was allowed to train for ~80 minutes. While it's theoretically possible that the accuracy could improve, time invested must be considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
